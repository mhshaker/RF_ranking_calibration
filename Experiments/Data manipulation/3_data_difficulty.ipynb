{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How changing the difficulty of a dataset effects the calibration methods\n",
    "# Change difficulty of generated data such that the RF atcheaves ACC in a range betwean 100% to 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.append('../../') # to access the files in higher directories\n",
    "sys.path.append('../') # to access the files in higher directories\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import Data.data_provider as dp\n",
    "import core as cal\n",
    "from estimators.IR_RF_estimator import IR_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "calib_methods = [\"RF\", \"Platt\" , \"ISO\", \"Rank\", \"CRF\", \"VA\", \"Beta\", \"Elkan\", \"tlr\"]\n",
    "# calib_methods = [\"RF\", \"Platt\" ]\n",
    "metrics = [\"acc\", \"auc\", \"brier\", \"ece\", \"logloss\", \"tce\"]\n",
    "\n",
    "data_name = \"S_difficulty\"\n",
    "\n",
    "params = {\n",
    "    \"runs\": 50,\n",
    "    \"data_difficulty\": [10, 20, 40, 60, 80, 100], # as percentage of the x_calib data\n",
    "    \"data_size\": 1000,\n",
    "    \"n_features\": 40,\n",
    "    \"n_estimators\": 100,\n",
    "    \"oob\": False,\n",
    "    \"test_split\": 0.3,\n",
    "    \"calib_split\": 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "X, y, tp = dp.make_classification_gaussian_with_true_prob(params[\"data_size\"], params[\"n_features\"], seed)\n",
    "data = cal.split_train_calib_test(\"diff_test\", X, y, params[\"test_split\"], params[\"calib_split\"], seed)\n",
    "irrf = IR_RF(n_estimators=params[\"n_estimators\"], oob_score=params[\"oob\"], random_state=seed)\n",
    "irrf.fit(data[\"x_train\"], data[\"y_train\"])\n",
    "acc = irrf.score(data[\"x_test\"], data[\"y_test\"])\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "calib_results_dict = {}\n",
    "\n",
    "for exp_data_difficulty in params[\"data_difficulty\"]:\n",
    "\n",
    "    # Data\n",
    "    exp_data_name = str(exp_data_difficulty) # data_name + \"_\" + \n",
    "    data_list.append(exp_data_name)\n",
    "    X, y, tp = dp.make_classification_gaussian_with_true_prob(params[\"data_size\"], params[\"n_features\"], 0)\n",
    "\n",
    "    data_dict = {} # results for each data set will be saved in here.\n",
    "    for seed in range(params[\"runs\"]): # running the same dataset multiple times\n",
    "        # split the data\n",
    "        data = cal.split_train_calib_test(exp_data_name, X, y, params[\"test_split\"], params[\"calib_split\"], seed)\n",
    "\n",
    "        # train model\n",
    "        irrf = IR_RF(n_estimators=params[\"n_estimators\"], oob_score=params[\"oob\"], random_state=seed)\n",
    "        irrf.fit(data[\"x_train\"], data[\"y_train\"])\n",
    "\n",
    "        # calibration\n",
    "        res = cal.calibration(irrf, data, calib_methods, metrics) # res is a dict with all the metrics results as well as RF probs and every calibration method decision for every test data point\n",
    "        data_dict = cal.update_runs(data_dict, res) # calib results for every run for the same dataset is aggregated in data_dict (ex. acc of every run as an array)\n",
    "    calib_results_dict.update(data_dict) # merge results of all datasets together\n",
    "tables = cal.mean_and_ranking_table(calib_results_dict, metrics, calib_methods, data_list, mean_and_rank=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = cal.mean_and_ranking_table(calib_results_dict, metrics, calib_methods, data_list, mean_and_rank=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calib_methods = calib_methods\n",
    "plot_calib_methods.remove(\"VA\")\n",
    "plot_calib_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    ax = tables[metric][plot_calib_methods].plot()\n",
    "    ax.set_xlabel(\"Calib_size\")\n",
    "    ax.set_ylabel(metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
