{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read saved results (tables) of old runs\n",
    "# combine multiple runs\n",
    "# make significance plots\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.append('../../') # to access the files in higher directories\n",
    "sys.path.append('../') # to access the files in higher directories\n",
    "import pandas as pd\n",
    "import core_tools as ct\n",
    "import core_calib as cal\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = './Data manipulation/results'\n",
    "exp_names = [\"1714153141_Real30_CTfix4\", \"1714161293_CT_L1\"]\n",
    "# exp_names = [\"1713869768_calibsize_final_more_run\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # n_features exp\n",
    "\n",
    "# path = f\"{main_path}/{exp_names[0]}\"\n",
    "# dict_data, tables, params = ct.read_results(path)\n",
    "\n",
    "# path = f\"results/{params['exp_name']}/features\"\n",
    "# if not os.path.exists(path):\n",
    "#     os.makedirs(path)\n",
    "\n",
    "# max_value = [None, 0.4, 0.125, 2, 0.3]\n",
    "# min_value = [None, None, None, None, 0.00]\n",
    "\n",
    "# for metric, max_v, min_v in zip(params[\"metrics\"], max_value, min_value): #\n",
    "#     df = tables[metric][params[\"calib_methods\"]].copy()\n",
    "\n",
    "#     # # remove noize by normalization\n",
    "#     # base = np.array(df[\"RF_d\"])\n",
    "#     # ref = base[0]\n",
    "#     # for i in range(len(base)):\n",
    "#     #     base[i] = base[i] / ref\n",
    "#     # for col_name in plot_calib_methods:\n",
    "#     #     df[col_name] = df[col_name] / base\n",
    "    \n",
    "#     # print(df)\n",
    "#     # ax = tables[metric][plot_calib_methods].plot()\n",
    "#     ax = df.plot(color=params[\"calib_method_colors\"])\n",
    "#     ax.set_xlabel(\"Number of Features\")\n",
    "\n",
    "#     if metric == \"acc\":\n",
    "#         metric_p = \"ACC\"\n",
    "#     elif metric == \"logloss\":\n",
    "#         metric_p = \"LogLoss\"\n",
    "#     elif metric == \"ece\":\n",
    "#         metric_p = \"ECE\"\n",
    "#     elif metric == \"brier\":\n",
    "#         metric_p = \"Brier\"\n",
    "#     elif metric == \"tce\":\n",
    "#         metric_p = \"TCE\"\n",
    "#     ax.set_ylabel(metric_p)\n",
    "#     plt.ylim(min_v, max_v)\n",
    "#     plt.savefig(f\"{path}/f_{metric}.pdf\", format='pdf', transparent=True)\n",
    "#     plt.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calib size exp\n",
    "\n",
    "# path = f\"{main_path}/{exp_names[0]}\"\n",
    "# dict_data, tables, params = ct.read_results(path)\n",
    "\n",
    "# path = f\"results/{params['exp_name']}/calib_size_zoom\"\n",
    "# if not os.path.exists(path):\n",
    "#     os.makedirs(path)\n",
    "\n",
    "# max_value = [0.81, 0.2, 0.15, 4, 0.15]\n",
    "# min_value = [0.72, 0.14, 0.025, 0, 0.015]\n",
    "\n",
    "# # max_value = [None, None, None, None, None]\n",
    "# # min_value = [None, None, None, None, None]\n",
    "\n",
    "# for metric, max_v, min_v in zip(params[\"metrics\"], max_value, min_value): #\n",
    "#     ax = tables[metric][params[\"calib_methods\"]].plot(color=params[\"calib_method_colors\"])\n",
    "#     if metric == \"acc\":\n",
    "#         metric_p = \"ACC\"\n",
    "#     elif metric == \"logloss\":\n",
    "#         metric_p = \"LogLoss\"\n",
    "#     elif metric == \"ece\":\n",
    "#         metric_p = \"ECE\"\n",
    "#     elif metric == \"brier\":\n",
    "#         metric_p = \"Brier\"\n",
    "#     elif metric == \"tce\":\n",
    "#         metric_p = \"TCE\"\n",
    "#     ax.set_xlabel(\"Calibration Set Size %\")\n",
    "#     ax.set_ylabel(metric_p)\n",
    "#     plt.ylim(min_v, max_v)\n",
    "#     plt.savefig(f\"{path}/cs_{metric}.pdf\", format='pdf', transparent=True)\n",
    "#     plt.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to make latex tables of metrics\n",
    "# for exp in exp_names:\n",
    "#     path = f\"{main_path}/{exp}\"\n",
    "#     dict_data, tables, params = ct.read_results(path)\n",
    "#     tables = ct.add_rank_mean(tables)\n",
    "#     ct.save_metrics_to_latex(tables, params[\"metrics\"], path, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = {\n",
    "#         \"RF_opt\": \"orange\", \n",
    "#         \"RF_large\": \"red\",\n",
    "#         \"DT\": \"forestgreen\", \n",
    "#         \"LR\": \"lightgray\", \n",
    "#         \"NN\": \"black\", \n",
    "#         \"SVM\": \"dimgrey\", \n",
    "#         \"GNB\": \"gray\",\n",
    "#     }\n",
    "\n",
    "# # colors = {\n",
    "# #         \"RF_d\": \"blue\", \n",
    "# #         \"RF_opt\": \"orange\", \n",
    "# #         \"RF_large\": \"red\",\n",
    "# #         \"CT\": \"slategray\",\n",
    "# #         \"RF_opt_CT\": \"black\",\n",
    "# #         \"Platt\": \"Brown\", \n",
    "# #         \"ISO\": \"purple\", \n",
    "# #         \"Beta\": \"magenta\", \n",
    "# #         \"PPA\": \"olive\", \n",
    "# #         \"VA\": \"gray\",\n",
    "# #         \"Rank\": \"silver\"     \n",
    "# # }\n",
    "\n",
    "# # to make CD plots for one stand alone exp\n",
    "# for exp in exp_names:\n",
    "#     path = f\"{main_path}/{exp}\"\n",
    "#     dict_data, tables, params = ct.read_results(path)\n",
    "#     tables = ct.add_rank_mean(tables)\n",
    "#     ct.res_statistics(tables, params[\"metrics\"], path, colors=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to combine Calib and OOB runs in one CD plot\n",
    "# tables_all = {}\n",
    "# for exp in exp_names:\n",
    "#     path = f\"{main_path}/{exp}\"\n",
    "#     dict_data, tables, params = ct.read_results(path)\n",
    "#     if len(tables_all) == 0: # for first exp\n",
    "#         tables_all = tables\n",
    "#     else: # combine tables # for second exp\n",
    "#         for table_key in tables.keys():\n",
    "#             col_names = np.array((tables[table_key].columns))\n",
    "#             col_names += \"_L\"\n",
    "#             tables[table_key].columns = col_names\n",
    "#             # print(tables[table_key])\n",
    "#             # print(f\"key {table_key} col_names\", col_names)\n",
    "#             tables_all[table_key] = pd.concat([tables_all[table_key], tables[table_key]], axis=1)\n",
    "#             # print(tables_all[table_key])\n",
    "\n",
    "# tables_all = ct.add_rank_mean(tables_all)\n",
    "# tables_all[\"brier\"]\n",
    "\n",
    "\n",
    "# ct.res_statistics(tables_all, params[\"metrics\"], \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc RF_d 0.8763433957642719\n",
      "acc RF_opt 0.9689469773963097\n",
      "acc RF_large 0.994225646632132\n",
      "acc Platt 0.9889896287238686\n",
      "acc ISO 0.9525370207256713\n",
      "acc Beta 0.9715454711135744\n",
      "acc VA 0.9370312066462065\n",
      "acc CT 0.9899206677836407\n",
      "acc PPA 0.9714422964850897\n",
      "acc Rank 0.9903036803060629\n",
      "brier RF_d 0.8789124422153622\n",
      "brier RF_opt 0.9649179746286161\n",
      "brier RF_large 0.9510512670437719\n",
      "brier Platt 0.9918424850096688\n",
      "brier ISO 0.9862088507274878\n",
      "brier Beta 0.9272299120472367\n",
      "brier VA 0.9790575509771858\n",
      "brier CT 0.9870231523092113\n",
      "brier PPA 0.9912194761027344\n",
      "brier Rank 0.9865030654435625\n",
      "ece RF_d 0.58220643818981\n",
      "ece RF_opt 0.9650926004832496\n",
      "ece RF_large 0.34295179929336783\n",
      "ece Platt 0.947277895981742\n",
      "ece ISO 0.7009191297010682\n",
      "ece Beta 0.8169423636165751\n",
      "ece VA 0.9069770594550334\n",
      "ece CT 0.5288787620403157\n",
      "ece PPA 0.3143205187867674\n",
      "ece Rank 0.9274331044758783\n",
      "logloss RF_d 6.3101695911502125e-06\n",
      "logloss RF_d PPP\n",
      "logloss RF_opt 0.8577873586979277\n",
      "logloss RF_large 0.09265399408275984\n",
      "logloss Platt 0.9697089351624684\n",
      "logloss ISO 0.9440970473857853\n",
      "logloss Beta 0.4409240242464675\n",
      "logloss VA 0.9696098767566316\n",
      "logloss CT 0.01094243198864465\n",
      "logloss CT PPP\n",
      "logloss PPA 0.01388190265275007\n",
      "logloss PPA PPP\n",
      "logloss Rank 0.9772712690774251\n",
      "time RF_d 0.9979388666580169\n",
      "time RF_opt 0.984632845480536\n",
      "time RF_large 0.9880781639466966\n",
      "time Platt 0.9670364177195976\n",
      "time ISO 0.9670448742166534\n",
      "time Beta 0.967079413724368\n",
      "time VA 0.9670519166561083\n",
      "time CT 0.9751444027604008\n",
      "time PPA 0.9676602343737127\n",
      "time Rank 0.9670753475653668\n"
     ]
    }
   ],
   "source": [
    "# # to combine Calib and OOB runs in one CD plot\n",
    "# tables_all = {}\n",
    "# for exp in exp_names:\n",
    "#     path = f\"{main_path}/{exp}\"\n",
    "#     dict_data, tables, params = ct.read_results(path)\n",
    "#     if len(tables_all) == 0: # for first exp\n",
    "#         tables_all = tables\n",
    "#     else: # combine tables # for second exp\n",
    "#         for table_key in tables.keys():\n",
    "#             col_names = np.array((tables[table_key].columns))\n",
    "#             col_names += \"_L\"\n",
    "#             tables[table_key].columns = col_names\n",
    "#             # print(tables[table_key])\n",
    "#             # print(f\"key {table_key} col_names\", col_names)\n",
    "#             tables_all[table_key] = pd.concat([tables_all[table_key], tables[table_key]], axis=1)\n",
    "#             # print(tables_all[table_key])\n",
    "\n",
    "# tables_all = ct.add_rank_mean(tables_all)\n",
    "# tables_all[\"brier\"]\n",
    "\n",
    "\n",
    "# ct.res_statistics_ttest(tables_all, params[\"metrics\"], \".\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RFcalib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
