{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.append('../../') # to access the files in higher directories\n",
    "sys.path.append('../') # to access the files in higher directories\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import Data.data_provider as dp\n",
    "import core as cal\n",
    "from estimators.IR_RF_estimator import IR_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "calib_methods = [\"RF\", \"Platt\" , \"ISO\", \"Rank\", \"CRF\", \"VA\", \"Beta\", \"Elkan\", \"tlr\"]\n",
    "metrics = [\"acc\", \"auc\", \"brier\", \"ece\", \"logloss\"]\n",
    "\n",
    "# data_name = \"S_trees\"\n",
    "data_name = \"spambase\"\n",
    "\n",
    "params = {\n",
    "    \"runs\": 50,\n",
    "    \"n_estimators\": 10, \n",
    "    \"data_size\": 1000,\n",
    "    \"n_features\": 20,\n",
    "    \"oob\": [False, True],\n",
    "    \"test_split\": 0.3,\n",
    "    \"calib_split\": 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "calib_results_dict = {}\n",
    "\n",
    "for exp_trees in params[\"n_tree\"]:\n",
    "\n",
    "    # Data\n",
    "    exp_data_name = str(exp_trees) # data_name + \"_\" + \n",
    "    data_list.append(exp_data_name)\n",
    "    # X, y, tp = dp.make_classification_gaussian_with_true_prob(params[\"data_size\"], params[\"n_features\"], 0)\n",
    "    X, y = dp.load_data(data_name, \"../../\")\n",
    "    \n",
    "    data_dict = {} # results for each data set will be saved in here.\n",
    "    for seed in range(params[\"runs\"]): # running the same dataset multiple times\n",
    "        # split the data\n",
    "        data = cal.split_train_calib_test(exp_data_name, X, y, params[\"test_split\"], params[\"calib_split\"], seed)\n",
    "        # train model\n",
    "        irrf = IR_RF(n_estimators=params[\"n_estimators\"], oob_score=params[\"oob\"], random_state=seed)\n",
    "        irrf.fit(data[\"x_train\"], data[\"y_train\"])\n",
    "        # calibration\n",
    "        res = cal.calibration(irrf, data, calib_methods, metrics) # res is a dict with all the metrics results as well as RF probs and every calibration method decision for every test data point\n",
    "        data_dict = cal.update_runs(data_dict, res) # calib results for every run for the same dataset is aggregated in data_dict (ex. acc of every run as an array)\n",
    "    calib_results_dict.update(data_dict) # merge results of all datasets together\n",
    "    \n",
    "tables = cal.mean_and_ranking_table(calib_results_dict, metrics, calib_methods, data_list, mean_and_rank=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calib_methods = calib_methods\n",
    "plot_calib_methods.remove(\"VA\")\n",
    "plot_calib_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    ax = tables[metric][plot_calib_methods].plot()\n",
    "    ax.set_xlabel(\"#trees\")\n",
    "    ax.set_ylabel(metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RFcalib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
